{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Stock Volatility Prediction (Models)\n",
    "### in this project , I will predict stock volatility by using \n",
    "### GARCH model\n",
    "### Linear model\n",
    "### non-linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arch import arch_model \n",
    "from sklearn.metrics import r2_score\n",
    "from stock_data_reader import StockDataReader\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM\n",
    "from keras.losses import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reader = StockDataReader('7az7vgtTPxiyj2Ncsc-H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Construct dataset\n",
    "### Construct dataset for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe for a simple linear regression, this part is only for the spark sample\n",
    "# yesterday's vol, yesterday's return\n",
    "def linear_data(ticker,window,thresh_date,export=False):\n",
    "    reader.initialize_data(ticker,'2005-01-01','2016-12-31')\n",
    "    dat = reader.price_table\n",
    "    dat = (dat.set_index(dat['date'])).drop(['date','adj_volume'],axis=1)\n",
    "    dat['return'] = (np.log(dat['adj_close'])-np.log((dat['adj_close']).shift(1)))*100\n",
    "\n",
    "    sigma_column_name = str(window)+'dayvar'\n",
    "    dat[sigma_column_name]= dat['return'].rolling(window,center=False).var()\n",
    "    if export==True:\n",
    "        return dat\n",
    "    dat_yesterday = dat[['return',sigma_column_name]].shift(1)\n",
    "    dat_yesterday_column_name = [x+'_yesterday' for x in list(dat_yesterday.columns)]\n",
    "    dat_yesterday.columns = dat_yesterday_column_name\n",
    "    data_linear = (pd.concat([dat,dat_yesterday],\n",
    "                            axis=1)).drop(['adj_close','return'],axis=1)\n",
    "    return data_linear[thresh_date:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dataset for other high-dimension models\n",
    "def get_data(ticker,time_window,sigma_window,thresh_date):\n",
    "    dat=linear_data(ticker,sigma_window,'Null',True)\n",
    "    sigma_column_name = str(sigma_window)+'dayvar'\n",
    "    dat_shift = dat[['return',sigma_column_name]]\n",
    "    columns = list(dat_shift.columns)\n",
    "    dat_copy = dat.copy()\n",
    "    for day in range(1,time_window+1):\n",
    "        dat_tmp = dat_shift.shift(day)\n",
    "        dat_tmp.columns = [name+'_'+str(day)+'daybefore' for name in columns]\n",
    "        dat_copy = pd.concat([dat_copy,dat_tmp],axis=1)\n",
    "    data_rnn = (dat_copy.drop(['adj_close','return'],axis=1))[thresh_date:]\n",
    "    return data_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class data_split(object):\n",
    "    \"\"\"this class is for train-validation-testing split\"\"\"\n",
    "    def __init__(self):\n",
    "        print('return form: y_train,x_train,(y_vali,x_vail),y_test,x_test')\n",
    "        pass\n",
    "    def train_test_split(self,dat,split_date):\n",
    "        \"\"\"I didn't put a validation set here because:\n",
    "        1: there is no need to tune hyper parameter\n",
    "        2: dataset is small ,I want to use more data\n",
    "        \"\"\"\n",
    "        train = dat[:split_date]\n",
    "        test  = dat[split_date:]\n",
    "        return train.iloc[:,0],train.iloc[:,1:],test.iloc[:,0],test.iloc[:,1:]\n",
    "    def train_validate_test_split(self,dat,split1,split2):\n",
    "        train = dat[:split1]\n",
    "        validate = dat[split1:split2]\n",
    "        test = dat[split2:]\n",
    "        return train.iloc[:,0],train.iloc[:,1:],validate.iloc[:,0],validate.iloc[:,1:],test.iloc[:,0],test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return form: y_train,x_train,(y_vali,x_vail),y_test,x_test\n"
     ]
    }
   ],
   "source": [
    "split = data_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### GARCH(1,1) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def garch(ticker,p_=1,q_=1):\n",
    "    reader.initialize_data('AAPL','2006-01-01','2016-12-31')\n",
    "    price_dat = reader.price_table\n",
    "    ret = (np.log(price_dat['adj_close'])-np.log((price_dat['adj_close']).shift(1)))*100\n",
    "    price_dat['returns']=ret\n",
    "    price_dat = (price_dat.set_index(price_dat['date']))['returns']\n",
    "    train_garch = price_dat['2009-01-01':'2016-01-01']\n",
    "    am = arch_model(train_garch, vol='Garch', p=p_, o=0, q=q_, dist='Normal')\n",
    "    res = am.fit(update_freq=5)\n",
    "    w = res.params['omega']\n",
    "    a = res.params['alpha[1]']\n",
    "    b = res.params['beta[1]']\n",
    "    dat_test=linear_data(ticker,21,'2006-01-01','2016-12-31')\n",
    "    for i in range(1,501):# here i created 500 paths for random walks, for every path I calculated the predicted var, then take average of them\n",
    "        dat_test['predict_var_%d'%(i)]=(a*((np.random.randn(len(dat_test)))**2)+b)*dat_test['21dayvar_yesterday']\n",
    "    predict = (dat_test.iloc[:,3:]).mean(axis=1)\n",
    "    r2 = r2_score(dat_test['21dayvar'],predict)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rf(ticker,window_=10):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    rf = RandomForestRegressor(max_depth=5)\n",
    "    dat_rf = get_data(ticker,window_,21,'2009-01-01')\n",
    "    y_train,x_train,y_test,x_test = split.train_test_split(dat_rf,'2016-01-01')\n",
    "    rf.fit(x_train,y_train)\n",
    "    r2 = rf.score(x_test,y_test)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rnn(ticker,window_=10):\n",
    "    dat_rf = get_data(ticker,window_,21,'2009-01-01')\n",
    "    y_train,x_train,y_test,x_test = split.train_test_split(dat_rf,'2016-01-01')\n",
    "    xtr = np.array(x_train)\n",
    "    ytr = np.array(y_train)\n",
    "    xte = np.array(x_test)\n",
    "    yte = np.array(y_test)\n",
    "    xtr_shape = xtr.shape\n",
    "    xte_shape = xte.shape\n",
    "    xtr = xtr.reshape(xtr_shape[0],window_,2)\n",
    "    xte = xte.reshape(xte_shape[0],window_,2)\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,LSTM\n",
    "    from keras.losses import mean_squared_error\n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(32,input_shape = (window_,2)))\n",
    "    lstm.add(Dense(32,activation = 'relu'))\n",
    "    lstm.add(Dense(16,activation = 'relu'))\n",
    "    lstm.add(Dense(1,activation='relu'))\n",
    "    lstm.compile(loss = mean_squared_error,optimizer='adam')\n",
    "    lstm.fit(xtr,ytr,batch_size=15,epochs=10)\n",
    "    lstm_result = lstm.predict(xte,batch_size=1).reshape(yte.shape[0],)\n",
    "    r2_lstm = r2_score(yte,lstm_result)\n",
    "    return r2_lstm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
